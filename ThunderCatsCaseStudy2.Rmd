---
title: "Thundercats Case Study 2"
author: "TEAM THUNDERCATS DDS SMU"
date: "7/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(sqldf)
library(dplyr)
library(VGAM)
library(caret)
library(nnet)
```
```{r}
test_train_split = 0.4
```
## UDFS

  - loadData: Load the data

  - generate_interaction_string: Function for generating interaction terms
  
  - build_model_string: Generates contrast equation for regression
  
  - run_linear_model: builds/test/summarizes a linear regression model using VGAM
  
  - run_neural_model: builds/test/summarizes a neuralnetwork model using nnet
```{r, echo=FALSE}
loadData <-function(){
  attrition_data = read.csv('atttrition.csv', stringsAsFactors=TRUE)
  drop_cols = c('Over18', 'EmployeeNumber', 'EmployeeCount', 'StandardHours')
  attrition_data[,drop_cols] = list(NULL)

  #potentially biasing Observation(s)?
  attrition_data = sqldf("SELECT * FROM attrition_data WHERE JobRole <> 'Research Director'")
  
  #hacky - sqlDF is fucking up my colnames
  names(attrition_data)[1] = 'Age'
  return(attrition_data)
  #attrition_data$Education = factor(attrition_data$Education)
}
#helper functions
generate_interaction_string <- function(columns){
    #f = paste(columns, ':')
    f = c()
    for(x in columns)
      for(y in columns)
      {
        if(x != y)
          if(!is.element(glue::glue('{y}:{x}'),f))
            f = append(f, glue::glue('{x}:{y}'))
      }
    return(f)
}

build_model_string <- function(target, columns, interactions=c()){
    
    model = c(columns, interactions)
    target_string = glue::glue("{target} ~ ")
    model_string = paste(model, collapse="+")
    f = target_string + model_string
    return(f)
}

run_linear_model <- function(data, target, columns, interactions=c()){
    model = c(columns, interactions)
    f = build_model_string(target, columns, interactions)
    
    sample = select(data, columns, target)
    
    
    #Train Test Split
    seed = 1234
    sample_size = floor(test_train_split*nrow(sample))
    set.seed(seed)
    train_set = sample(seq_len(nrow(sample)), size=sample_size)
    
    train = sample[train_set,]
    test = sample[-train_set,]
    
    
    #Build the model
    linear_model <-vglm(as.formula(f),family = "multinomial",data=train)
    
    
    #Summarize the model
    print(summary(linear_model))
    
    #Run Predictions
    x<-select(test, -target)
    y<-select(test, target)
    
    probability<-predict(linear_model,x,type="response")
    test$pred_log_reg<-apply(probability,1,which.max)
    test$pred_log_reg[which(test$pred_log_reg=="1")]<-levels(test[,target])[1]
    test$pred_log_reg[which(test$pred_log_reg=="2")]<-levels(test[,target])[2]
    
    
    str(f)
    #Accuracy of the model
    mtab<-table(test$pred_log_reg,test[,target])
    confusionMatrix(mtab)
}
run_nn_model <- function(data, target, columns, interactions=c()){
    model = c(columns, interactions)
    f = build_model_string(target, columns, interactions)
    
    sample = select(data, columns, target)
    
    
    #Train Test Split
    seed = 1234
    sample_size = floor(test_train_split*nrow(sample))
    set.seed(seed)
    train_set = sample(seq_len(nrow(sample)), size=sample_size)
    
    train = sample[train_set,]
    test = sample[-train_set,]
    
    
    #Build the model
    neural_model <-nnet(as.formula(f),data=train,size = 4,decay = 0.0001,maxit = 500)
    
    
    #Summarize the model
    print(summary(neural_model))
    
    #Run Predictions
    x<-select(test, -target)
    y<-select(test, target)
    
    #Predict using the model
    test$pred_nnet<-predict(neural_model,x,type='class')
    str(f)
    #Accuracy of the model
  
    mtab<-table(test$pred_nnet,test[,target])
    print(summary(mtab))
    confusionMatrix(mtab)
    
    
  
   
}
#helper function for quick viz of percentage of employees that have left
attrition_by_category <- function(category, data){

sql_statement = glue::glue('SELECT 
                                  {category}
                                  ,Attrition
                                  ,COUNT(*)*1.0  as Count
                             FROM data 
                             GROUP BY {category}
                                     , Attrition')  
  
counts = sqldf(sql_statement)
yes =sqldf("SELECT * FROM counts WHERE Attrition = 'Yes'")
no =sqldf("SELECT * FROM counts WHERE Attrition = 'No'")
attrition_rates = sqldf(glue::glue('SELECT
          y.{category},
          y.Count as Departed,
          n.Count as Employed,
          (y.Count + n.Count) as Total,
          (y.Count/(n.Count + y.Count)) as AttritionRate
       FROM yes y
       LEFT JOIN no n ON n.{category} = y.{category}'))
print(attrition_rates)
return(ggplot(counts, aes(counts[,category], counts$Count)) + geom_bar(aes(fill = Attrition), 
   width = 0.4, position = position_dodge(width=0.5), stat="identity") +  
   theme(legend.position="top", legend.title = 
   element_text(),axis.title.x=element_text(), 
   axis.title.y=element_text()))
}

#helper function for extracting col names
#TODO: add cols that are factors but also ints 
factor_columns <- function(dataFrame){
  return(colnames(dataFrame[,sapply(dataFrame, is.factor) & colnames(dataFrame) != "id"]))
}

non_factor_columns <- function(dataFrame){
  return(colnames(dataFrame[, !sapply(dataFrame, is.factor) & colnames(dataFrame) != "id"]))
}


plotSatisfaction <- function(category, data){
  sql = glue::glue('SELECT
                   {category},
                   AVG(satisfaction_index) as AverageHappiness
                   FROM data
                   GROUP BY {category}')
  happiness = sqldf(sql)
  return(ggplot(happiness, aes(happiness[,category], happiness$AverageHappiness)) + geom_bar(aes(fill = category), 
   width = 0.4, position = position_dodge(width=0.5), stat="identity") + coord_flip() +  
   xlab(category) + ylab('Happiness')) 
}
```

## EDA Exploratory Questions
  
   # Job Satisfaction
    
   As a proxy for happiness with current employment, we will construct an index = EnvironmentSatisfaction + JobInvolvement + JobSatisfaction (tried this - didn't do much) and look for trends across the categorical variables.
   
```{r, echo=FALSE}
satisfaction_data = loadData()
satisfaction_data = sqldf('SELECT s.*,  
                            JobSatisfaction as satisfaction_index
                            FROM satisfaction_data s')

for(x in factor_columns(satisfaction_data)){
  if(x != 'Attrition'){  print(plotSatisfaction(x, satisfaction_data))
  }
}
non_factor_columns(satisfaction_data)
for(x in non_factor_columns(satisfaction_data)){
  average_happiness = sqldf(glue::glue('SELECT 
                                       {x},
                                       AVG(satisfaction_index) as average_happiness
                                       FROM satisfaction_data 
                                       GROUP BY {x}'))
  print(ggplot(data=average_happiness, aes(x=glue::glue('{x}'), y='average_happiness')) + geom_point()
        + xlab(x))
  #qplot(satisfaction_data[,c(x, 'satisfaction_index')],  binwidth = 1.0, geom = "histogram", xlab = x, ylab = "satisfaction_index",
  #    y = ..density.., fill = I("white"), colour = I("black")) +
  #stat_density(geom = "line")
}
```
## Analysis:
  
    Job satisfaction as measured by the JobSatisfaction field alone seems somewhat constant across categorical variables.
    By job role, healthcare reps and research scientists seems to be most satisfied with their job.
  
## Variable Ranks
  
  #Colinearity checks?
```{r}
attrition_data = loadData()
#let's partiton by attrition and look at the data a bit
left_data = sqldf("SELECT * FROM attrition_data WHERE Attrition = 'Yes'")
stayed_data = sqldf("SELECT * FROM attrition_data WHERE Attrition = 'No'")
histogram_fields = c('Age', 'HourlyRate', 'MonthlyRate', 'DailyRate', 'DistanceFromHome', 'MonthlyIncome', 'PercentSalaryHike', 'TotalWorkingYears', 'YearsWithCurrentManager')
for(field in histogram_fields)
{
    print(ggplot(data=left_data, aes(left_data[,field])) + geom_histogram())
    #print(ggplot(data=stayed_data, aes(stayed_data[,field]) + geom_histogram()))
}

```



# Analysis:
  
  - Overtime seems to be an important factor when considering attrition rate. OF the 416 employees that worked overtime, nearly 30% of them left their positions.
  
```{r}
experiment_data = loadData()
experiment_target = c('Attrition')
experiment_columns = c(
                          'BusinessTravel'
                        , 'Department'
                        , 'DistanceFromHome'
                        , 'EducationField'
                        , 'EnvironmentSatisfaction'
                        , 'OverTime'
                        , 'Gender'
                        , 'HourlyRate'
                        , 'DailyRate'
                        , 'JobInvolvement'
                        , 'JobLevel'
                        , 'Education'
                        , 'JobSatisfaction'
                        , 'MaritalStatus'
                        , 'MonthlyIncome'
                        , 'MonthlyRate'
                        , 'NumCompaniesWorked'
                        , 'PerformanceRating'
                        , 'PercentSalaryHike'
                        , 'JobRole'
                        , 'Age'
                        , 'RelationshipSatisfaction'
                        , 'StockOptionLevel'
                        , 'TotalWorkingYears'
                        , 'WorkLifeBalance'
                        , 'TrainingTimesLastYear'
                        , 'YearsAtCompany'
                        , 'YearsInCurrentRole'
                        , 'YearsSinceLastPromotion'
                        , 'YearsWithCurrManager')

experiment_interactions = generate_interaction_string(experiment_columns[2])
run_linear_model(experiment_data, experiment_target, experiment_columns, experiment_interactions)

#run_nn_model(experiment_data, experiment_target, experiment_columns, experiment_interactions)
```
## Full model analysis

    Accuracy of 84.17% vs target balance of {}
    
    
    Statistically Significant Features:
    
    Travel, DistanceFromHome, EnvironmentSatisfaction, OverTime, JobInvolvement, JobLevel, 
    JobSatisfaction, MaritalStatus, NumCompaniesWorked, WorkLifeBalance, YearsInCurrentRole,
    YearsSinceLastPromotion, YearsWithCurrManager

## Let's try a reduced model

```{r}
experiment_data  =loadData()
experiment_target = c('Attrition')
experiment_columns = c(
                          'BusinessTravel'
                        , 'DistanceFromHome'
                        , 'EnvironmentSatisfaction'
                        , 'OverTime'
                        , 'JobInvolvement'
                        , 'JobLevel'
                        , 'JobSatisfaction'
                        , 'MaritalStatus'
                        , 'NumCompaniesWorked'
                        , 'WorkLifeBalance'
                        , 'YearsInCurrentRole'
                        , 'YearsSinceLastPromotion'
                        , 'YearsWithCurrManager')

experiment_interactions =c()# c('JobLevel:YearsSinceLastPromotion:EnvironmentSatisfaction')
run_linear_model(experiment_data, experiment_target, experiment_columns, experiment_interactions)
run_nn_model(experiment_data, experiment_target, experiment_columns, experiment_interactions)


```

## Check target balances by factor

```{r}
experiment_data = loadData()
experiment_factor_columms = c(
                          'BusinessTravel'
                        , 'EnvironmentSatisfaction'
                        , 'OverTime'
                        , 'JobInvolvement'
                        , 'JobLevel'
                        , 'JobSatisfaction'
                        , 'MaritalStatus'
                        , 'WorkLifeBalance')


for(f in experiment_factor_columms){
  print(attrition_by_category(f, experiment_data))
}

```

